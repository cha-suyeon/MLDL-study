# AIFFEL DeepML Flipped School
Facilitator: ì°¨ìˆ˜ì—°
- Date: 2022.01.10-2022.02.23
- Members: ê¹€í˜„ì§€, ë°•í¥ì„ , ì‹ ì€ì •, ì•ˆì •í¬, ì´ë‹¤ì •, ì´ì˜ˆì˜, ì¥ë³‘ìš©, ì¡°ì†¡í¬, í•¨íƒœì‹

</br>

## Introduction

This `Flipped School` study is for people like this

```
1) Beginners who are new to Deep Learning
2) Intermediate level knowledge of Deep Learning but want to learn in-depth
3) Seniors who want to look at the trends of Deep Learning
```

> Effective facilitation of a discussion involves the employment of different perspectives and different skills to create an inclusive environment.
I tried to consider the features of effective discussions, amd conditions that promote small group interaction and engagement.


</br>

## Uploaded

> - Textbook [Posting Series](https://velog.io/@cha-suyeon/series/%ED%98%BC%EC%9E%90%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D)
> - cs231 [Posting Series](https://velog.io/@cha-suyeon/series/CS231n)
> - [Review](https://velog.io/@cha-suyeon/%ED%8D%BC%EC%8B%A4facilitator%EB%A1%9C%EC%84%9C%EC%9D%98-DeepML-%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-%ED%9B%84%EA%B8%B0)


</br>

## Questions & Answers

> I provided questions to the discussion and summarize the results in a visible way using Notion tools.

1. 1x1 Convolution layerë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? [Answer](https://github.com/cha-suyeon/MLDL-study/blob/main/answers/1x1_Convolution_layer.md)
2. Non-Linearity(ë¹„ì„ í˜•)ì˜ ì˜ë¯¸ì™€ í•„ìš”í•œ ì´ìœ  / tanhê°€ sigmoidë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ëŠ” ì´ìœ  / ReLUì˜ ë¬¸ì œì  [Answer](https://github.com/cha-suyeon/MLDL-study/blob/main/answers/Activation_Function.md)
3. Batch Normalizationê°€ ì™œ í•„ìš”í• ê¹Œìš”? /Batch Normalizationì˜ íš¨ê³¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”? [Answer](https://github.com/cha-suyeon/MLDL-study/blob/main/answers/Batch_Normalization.md)
4. ì™œ layerê°€ Conv layerì™€ Pooling Layerê°€ ë°˜ë³µë˜ë©° ë‚˜ì˜¬ê¹Œìš”?/ë§ˆì§€ë§‰ì—ëŠ” Fully Connected Layerë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë­˜ê¹Œìš”? [Answer](https://github.com/cha-suyeon/MLDL-study/blob/main/answers/CNN_Architecture.md)
5. CNNê³¼ RNNì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¼ê¹Œìš”?
6. ìˆ˜ë ´(`convergence`)ì„ ë¹¨ë¦¬ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€ìš”?
7. Downsamplingì„ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”? sizeê°€ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ paddingì„ í•˜ëŠ”ë°, ì™œ poolingì„ í†µí•´ ë˜ ì¤„ì´ëŠ” ê±¸ê¹Œìš”?
8. Dropoutì˜  íš¨ê³¼ / BN(Batch Normalization)ì´ ë‚˜ì˜¤ê³  ë‚˜ì„œ Dropoutì€ ì‹¤ì œë¡œ ì˜ ì•ˆ ì“°ì´ë‚˜ìš”? / Dropoutì€ ì£¼ë¡œ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë ê¹Œìš”?
9. Dropoutì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ì „ì²´ í•™ ìŠµì‹œê°„ì´ ëŠ˜ì–´ë‚˜ëŠ” ì´ìœ ë¡œ ê° ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸ë˜ëŠ” íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì´ë¼ê³  í•˜ì˜€ëŠ”ë°ìš”. íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“¤ë©´ í•™ìŠµì‹œê°„ì´ ì¤„ì–´ë“œëŠ” ê²Œ ì•„ë‹Œì§€ìš”?
10. searching spaceì— 3~4ê°œ ì´ìƒì˜ ì°¨ì›ì´ í¬í•¨ëœ ê²½ìš° ê·¸ë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. high spacesì—ì„œëŠ” ì•„ì£¼ ë‚˜ìœ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ”ë°ìš”. ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
11. Hidden layerë¥¼ í™œì„±í™” ì‹œí‚¤ëŠ” í•¨ìˆ˜ë¡œ sigmoidë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ReLUë¼ëŠ” í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ ì´ìœ ê°€ ë­˜ê¹Œìš”?
12. `learning rate` Î±ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ Î±ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ë‚˜ìš”?
13. LSTMì´ RNNì˜ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆê¹Œ? 
14. LSTMì—ëŠ” cell stateì™€ hidden stateê°€ ëª¨ë‘ ìˆìŠµë‹ˆë‹¤. cellë¡œ long-term dependencyë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤ê³  í•˜ëŠ”ë°ìš”. cell stateì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?
15. LSTMì— ë„ì…ëœ ì„¸ ê°€ì§€ gateì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?
16. feedback RNNê³¼ LSTM/GRUì˜ ì°¨ì´ì 
17. many-to-one RNN architectureë¥¼ ì ìš©í•˜ê¸° ë” ì¢‹ì€ task 2ê°œëŠ”?
18. ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‘ê²Œ í•  ë•Œì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
19. CNNê³¼ ë‹¨ìˆœ feed-forward neural networkë¥¼ ë¹„êµí•˜ë©´ ì–´ëŠ ìª½ì´ parameter ê°œìˆ˜ê°€ ë” ë§ì„ê¹Œìš”?
20. Poolingì€ overlap í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¼ê³  í•©ë‹ˆë‹¤. Poolingì„ ì ìš©í•  ë•Œ overlapí•˜ì—¬ ë” ì´˜ì´˜íˆ featureë¥¼ ë½‘ìœ¼ë©´ ì¢‹ì„ ê²ƒ ê°™ì€ë°, ê·¸ë ‡ê²Œ í•˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
21. RNNì—ì„œ activation functionìœ¼ë¡œ tanhë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?
22. `robust`í•˜ë‹¤ëŠ” ê²ƒì€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€ìš”?
23. Safety marginì˜ ê°œë…. ë³¸ ê°•ì˜ì—ì„œëŠ” safety marginì„ 1ë¡œ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ ì‚¬ì‹¤ ì´ ê°’ì€ ë³„ ìƒê´€ì´ ì—†ë‹¤ ë¼ê³  í•˜ì˜€ëŠ”ë° ê·¸ ì´ìœ ê°€ ë¬´ì—‡ì¼ê¹Œìš”?
24. SGD, RMSprop, Adamì— ëŒ€í•´ ì°¨ì´ì ì„ ë‘ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
25. SGDì—ì„œ Stochasticì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
26. í•´ë‹¹ ìŠ¬ë¼ì´ë“œì—ì„œ Car scoreê°€ cat scoreë³´ë‹¤ ë†’ì•„ì„œ lossê°€ 0ì´ ë©ë‹ˆë‹¤. ê·¸ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì¸ê°€ìš”? / SVM loss functionì„ ì•„ë˜ì™€ ê°™ì´ ì œê³±í•­ìœ¼ë¡œ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?
27. Transfer Learning & Fine Tuningì˜ ì°¨ì´ê°€ ë¬´ì—‡ì¼ê¹Œìš”?
28. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•„ìš”ì„± / 0ìœ¼ë¡œ ì´ˆê¸°í™” í•  ê²½ìš° / ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ê°€ ì•„ì£¼ í¬ê±°ë‚˜ ì‘ì„ ë•Œ
29. paddingì—ì„œ zero-paddingì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?

</br>

## Curriculum

|No.|Date|Topic|Textbook|Lecture|
|:--:|--|:--:|--|:--:|
|00|January 07, 2022|Introduction to Machine and Deep Learning/Data Handling|chapter 1, 2|âœ–|
|01|January 10, 2022|Image Classification|âœ–|[Lec 2.](https://velog.io/@cha-suyeon/CS231n-2%EA%B0%95-%EC%9A%94%EC%95%BD)|
|02|January 12, 20222|Regression Algorithm and Regularization|[chapter 3 -1](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-K-Nearest-Neighbors-R), [chapter 3 -2](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-feature-engineering-%EB%8B%A4%EC%A4%91-%ED%9A%8C%EA%B7%80)|âœ–|
|03|January 17, 2022|Loss Functions and Optimization|âœ–|[Lec 3.](https://velog.io/@cha-suyeon/cs231-Lecture-3-Loss-Functions-and-Optimization-%EC%9A%94%EC%95%BD)|
|04|January 19, 2022|Classification Algorithm|[chapter 4 -1](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-Logistic-Regression%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80), [chapter 4 -2](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-%EB%B0%B0%EC%B9%98%EC%99%80-%EB%AF%B8%EB%8B%88-%EB%B0%B0%EC%B9%98-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95)|âœ–|
|05|January 24, 2022|Introduction to Neural Networks|âœ–|[Lec 4.](https://velog.io/@cha-suyeon/CS231n-4%EA%B0%95-%EC%A0%95%EB%A6%AC-Introduction-to-Neural-Networks)|
|06|January 26, 2022|Deep Learning|[chapter 7](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8)|âœ–|
|07|February 7, 2022|Artificial Neural Networks for Image|[chapter 8](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D-Convolution-Neural-Network)|âœ–|
|08|February 9, 2022|Convolutional Neural Networks|âœ–|[Lec 5.](https://velog.io/@cha-suyeon/cs231n-5%EA%B0%95-%EC%A0%95%EB%A6%AC-Convolutional-Neural-Networks)|
|09|February 14, 2022|Training Neural Networks I|âœ–|[Lec 6.](https://velog.io/@cha-suyeon/cs231n-6%EA%B0%95-%EC%A0%95%EB%A6%AC-Training-Neural-Networks-I)|
|10|February 16, 2022|Training Neural Networks II|âœ–|[Lec 7.](https://velog.io/@cha-suyeon/cs231n-7%EA%B0%95-%EC%A0%95%EB%A6%AC-Training-Neural-Networks-II)|
|11|February 21, 2022|Artificial Neural Networks for Text|[chapter 9](https://velog.io/@cha-suyeon/%ED%98%BC%EA%B3%B5%EB%A8%B8-%EC%88%9C%EC%B0%A8-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80-%EC%88%9C%ED%99%98-%EC%8B%A0%EA%B2%BD%EB%A7%9D)|âœ–|
|12|February 23, 2022|Recurrent Neural Networks|âœ–| [Lec 10.](https://velog.io/@cha-suyeon/cs231n-10%EA%B0%95-%EC%A0%95%EB%A6%AC-Recurrent-Neural-Networks)|

</br>

## Reference

> - Material: 
>   - Textbook: `í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹+ë”¥ëŸ¬ë‹`
>     - ğŸ“™ Github: [Colab(Jupyter Notebook)](https://github.com/rickiepark/hg-mldl)
>     - ğŸ’» Youtube Link: [hg-mldl](http://bit.ly/hg-mldl-youtube)
>   - Video(youtube): `cs231n`
>     - Title: `cs231n`(Convolutional Neural Networks for Visual Recognition)
>     - ğŸ“’ Course Notes: [CS231n(Spring 2017)](http://cs231n.stanford.edu/)    
